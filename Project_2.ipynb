{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi Level Perceptron Model for Given Dataset","metadata":{}},{"cell_type":"markdown","source":"## Created by Sean, Sushmita, Vatsal","metadata":{}},{"cell_type":"markdown","source":"### Libraries Used :-","metadata":{}},{"cell_type":"code","source":"import re\nimport csv\nimport pandas as pd\nimport numpy as np\nimport math\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Function to read both input files to create Training Set, Validation Set, Test Set","metadata":{}},{"cell_type":"code","source":"def read_input_file(file_name,k):\n\n    feature_vectors = []\n    file = open(file_name,'r')\n\n    for line in file:\n        vectors = []\n        data = re.split(r'[()\\s]\\s*', line)\n        while '' in data:\n            data.remove('')\n\n        for item in data:\n            vectors.append(int(item))\n        feature_vectors.append(vectors)\n    if k==1:\n        with open(\"input.csv\",\"w+\") as my_csv:\n            csvWriter = csv.writer(my_csv,delimiter=',')\n            csvWriter.writerows(feature_vectors)\n    else:\n        with open(\"input_2.csv\",\"w+\") as my_csv:\n            csvWriter = csv.writer(my_csv,delimiter=',')\n            csvWriter.writerows(feature_vectors)        \nread_input_file('ClassifiedSetData.txt',1)\nread_input_file('TestSetData.txt',2)","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Class for creating MLP\n#### Functions :-\n<ol>\n    <li>init - To initialize required data</li>\n    <li>split_data - Splits Data into train, validate and test</li>\n    <li>multiplication_between_layers - calculates total input to a perceptron node by $\\sum_{i=1}^{n} x_i*w_i$</li>\n    <li>merge_data - Merge train and validate sets</li>\n    <li>logistic - Activation Function</li>\n    <li>forward_propagation - to classify based on inputs</li>\n    <li>backward_propagation - to Update Weight as per the result</li>\n    <li>accuracy - calculates accuracy of the model</li>\n    <li>confusion_matrix - Generates Confusion Matrix</li>\n    <li>confusion_matrix_create - preps matrix for beautiful display</li>\n</ol>","metadata":{}},{"cell_type":"code","source":"class MLP:\n    def __init__(self):\n        self.number_of_nodes=10\n        self.max_feature_value=96\n        self.num_output_nodes=8\n        self.True_Positive=[0]*8\n        self.True_Negative=[0]*8\n        self.False_Positive=[0]*8\n        self.False_Negative=[0]*8\n        \n    def split_data(self):\n        self.df=pd.read_csv('input.csv',header=None)\n        self.df1=pd.read_csv('input_2.csv',header=None)\n        self.df=self.df.rename({0:'ID',1:'Feature_1',2:'Feature_2',3:'Feature_3',4:'Feature_4',5:'Feature_5',6:'Feature_6',7:'Feature_7',8:'Feature_8',9:'Feature_9',10:'Feature_10',11:'Output'}, axis=1)\n        self.df1=self.df1.rename({0:'ID',1:'Feature_1',2:'Feature_2',3:'Feature_3',4:'Feature_4',5:'Feature_5',6:'Feature_6',7:'Feature_7',8:'Feature_8',9:'Feature_9',10:'Feature_10',11:'Output'}, axis=1)\n        self.train_data, self.validate_data, self.test_data = np.split(self.df.sample(frac=1,random_state=42), [int(.6*len(self.df)), int(.8*len(self.df))])\n    def multiplication_between_layers(self,weights, inputs):\n        total=0\n        for i in range(len(weights)):\n            total += (inputs[i] * weights[i])\n        return total   \n    def merge_data(self):\n        frames = [self.train_data,self.validate_data]\n        self.train_data=pd.concat(frames) \n    def logistic(self, x):\n        return 1 / ( 1 + math.exp(-x))\n    \n    def normalize_array(self, features,normalize_value):\n        normalized_array=[]\n        for i in features:\n            normalized_array.append(i/normalize_value)\n        return normalized_array\n    \n    def initialize_weight(self,x_dim,y_dim):\n        return np.random.randn(x_dim,y_dim)*np.sqrt(1/y_dim)\n    \n    def set_weights(self,Wh1,Wh2):\n        print(\"Initial Weight :\")\n        self.Wh1=Wh1\n        print(\"Hidden Layer Weight = \",self.Wh1)\n        self.Wh2=Wh2\n        print(\"Output Layer Weight = \",self.Wh2)\n        \n    def forward_propagation(self,training_data):\n            \n\n        self.hidden_layer = []\n        for i in range(self.number_of_nodes):\n            self.hidden_layer.append(self.logistic(self.multiplication_between_layers(self.Wh1[i], training_data)))\n\n        self.output_layer = []\n    \n        for i in range(self.num_output_nodes):\n            self.output_layer.append(self.logistic(self.multiplication_between_layers(self.Wh2[i], self.hidden_layer)))\n        return self.hidden_layer, self.output_layer\n \n    def back_propagation(self,learning_rate, min_accuracy):\n        epochs = 0\n        while self.accuracy(self.train_data) < min_accuracy:\n            epochs += 1\n            for value in self.train_data.values:\n                training_vector = self.normalize_array(list(value)[1:11], self.max_feature_value)\n                self.forward_propagation(training_vector)\n   \n                error_1= [] \n                for j in range(self.num_output_nodes):\n                    p = self.output_layer[j]\n                    learning_rate = 0.3\n                    if j == list(value)[-1]:\n                        learning_rate = 0.7\n                    error_1.append(p * (1 - p) * (learning_rate - p))\n  \n                error_2 = [] \n                for j in range(self.number_of_nodes):\n                    hidden_layer_input = self.hidden_layer[j]\n                    output_with_error = self.multiplication_between_layers(self.Wh2[:,j], error_1)\n                    error_2.append(hidden_layer_input* (1 - hidden_layer_input) * output_with_error)\n\n                for j in range(self.number_of_nodes):\n                    for i in range(self.num_output_nodes):\n                        self.Wh2[i][j] = self.Wh2[i][j] + learning_rate * error_1[i] * self.hidden_layer[j]\n                    for k in range(self.number_of_nodes):\n                        self.Wh1[j][k] = self.Wh1[j][k]   + learning_rate * error_2[j] * training_vector[k]\n            #print('finish',self.s) # Vatsal this for just testing \n                 \n        return epochs\n    def accuracy(self, input_data):\n        correct = 0\n        for data in input_data.values:\n            hidden_layer, output_layer = self.forward_propagation(self.normalize_array(list(data)[1:11], self.max_feature_value))\n            if (np.argmax(output_layer) == list(data)[-1]):\n                correct += 1\n        self.s=correct / len(input_data)\n        return correct / len(input_data)\n    def confusion_matrix(self,input_data):\n        number_of_correct=0\n        accuracy=0\n        for value in input_data.values:\n            training_vector = self.normalize_array(list(value)[1:11], self.max_feature_value)\n            hidden_layer,output_layer=self.forward_propagation(training_vector)\n            model_output_pred=np.argmax(output_layer)\n            actual_output=list(value)[-1]\n            if(model_output_pred == actual_output):\n                number_of_correct+=1\n                self.True_Positive[actual_output]+=1\n                j=0\n                while j<8:\n                    if j!=actual_output:\n                        self.True_Negative[j]+=1\n                    else:\n                        self.False_Positive[model_output_pred]+=1\n                        self.False_Positive[actual_output]+=1\n                        for i in range(8):\n                            if i!=model_output_pred or i!=actual_output:\n                                self.False_Negative[i]+=1\n                    j+=1\n        accuracy = number_of_correct / len(input_data)\n        return accuracy\n\n    def confusion_matrix_create(self):\n#         print(\"True Positive: \"+str(self.True_Positive))\n#         print(\"False Positive: \"+str(self.False_Positive))\n#         print(\"True Negative: \"+str(self.True_Negative))      \n#         print(\"False Negative: \"+str(self.False_Negative))\n        self.numpyArray = np.array([self.True_Positive,self.False_Positive,self.True_Negative,self.False_Negative]) \n  \n        # generating the Pandas dataframe \n        # from the Numpy array and specifying \n        # name of index and columns \n        self.confusion_matrix_final = pd.DataFrame( data = self.numpyArray,  \n                                                    index = [\"True_Positive\",\"False_Positive\",\"True_Negative\",\"False_Negative\"],  \n                                                    columns = [\"Class 0\",\"Class 1\", \"Class 2\",\"Class 3\",\"Class 4\", \"Class 5\",\"Class 6\", \"Class 7\"])\n\n\n\n\nd=MLP()\nd.split_data()\nWeights_input_layer=d.initialize_weight(10,10)\nWeights_hidden_layer=d.initialize_weight(8,10)\nd.merge_data()\nd.set_weights(Weights_input_layer,Weights_hidden_layer)\nprint(\"Number of epochs:\",d.back_propagation(0.3, 0.85))\nprint(\"Final Weights \")\nprint(\"Hidden Layer Weight = \",d.Wh1)\nprint(\"Output Layer Weight = \",d.Wh2)","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Initial Weight :\nHidden Layer Weight =  [[ 5.59433318e-01  1.47063978e-02 -2.30852603e-01 -1.76286939e-01\n  -1.49895760e-01  1.46541214e-01 -1.39187302e-01 -2.59482586e-01\n   4.25600745e-01 -2.82612253e-01]\n [-1.36309622e-01  3.03019775e-01  2.98731513e-01 -2.23445067e-01\n  -1.33695754e-01 -4.76434622e-01  1.38279869e-02 -3.38535685e-01\n  -8.24503438e-02 -2.94847969e-01]\n [-7.94496343e-02 -3.07266641e-01 -8.85902060e-02  3.27209886e-01\n   6.53717863e-02 -1.92627312e-01  3.34970134e-01  1.52696141e-01\n  -5.30042449e-01 -1.10474208e+00]\n [ 6.29696944e-02 -9.36265366e-02  4.24923068e-01 -4.80177710e-01\n   7.14514621e-02 -5.67175656e-03 -6.50769568e-02 -1.08220414e-02\n  -9.57538049e-02  8.14722380e-04]\n [-8.03001761e-02  3.69029398e-01 -1.88361144e-01  1.85806150e-01\n   1.93037091e-02 -6.64509958e-01 -2.30460953e-01  4.16203557e-01\n   3.23824738e-01  1.65614950e-02]\n [ 4.91401166e-01 -3.62213383e-01 -4.30082182e-01  4.68692705e-01\n  -6.89652518e-01  3.29139329e-01  2.08687892e-02  4.17278519e-01\n   4.35996630e-02 -2.30417334e-01]\n [-8.41157573e-02  2.23877922e-01  7.68813216e-02 -1.94624717e-01\n  -1.14313488e-01  1.08665615e-02 -1.87092571e-01 -2.01720833e-01\n  -3.46596643e-01 -2.16668726e-01]\n [ 1.60590721e-01 -3.98890540e-01 -4.33915740e-02  4.29933711e-02\n  -2.13433183e-01  6.55794061e-01 -4.31889659e-01  6.86414760e-02\n   9.45401692e-02  2.70670086e-01]\n [-9.55015370e-01  3.85897620e-01 -3.10889771e-01 -1.50663007e-01\n  -2.88419377e-01  9.85926103e-05 -2.08706218e-02  1.29339257e-01\n   7.19581660e-02  2.89374016e-01]\n [-1.49640174e-01  5.34894795e-02  4.89496472e-01 -2.69580351e-01\n   3.00285695e-01  3.22750150e-01  5.77210924e-01 -3.99239343e-01\n  -9.57159335e-02  1.63679303e-02]]\nOutput Layer Weight =  [[-0.77704849  0.16590801  0.09797724  0.27377367  0.03657968  0.42620153\n   0.39113326 -0.27482454 -0.4021463  -0.13713716]\n [ 0.10485274 -0.10394604 -0.05015433  0.24951887  0.22138541 -0.2378936\n   0.53645804  0.09444678 -0.01377127  0.21504902]\n [ 0.10217726  0.16196833  0.05421083 -0.35642205  0.04797011  0.16972125\n   0.48909951 -0.14610391  0.3528292   0.65324062]\n [-0.05115329  0.33938062  0.30728433 -0.09229036 -0.21927841  0.31546194\n   0.06311742 -0.04783692 -0.25173589  0.28017412]\n [-0.38512129  0.50998718  0.36718539  0.10988834  0.16129291  0.01578178\n  -0.25281255  0.05166626 -0.27921825  0.01273205]\n [-0.2586746   0.2810402  -0.47493044 -0.34610322  0.15067708  0.07041698\n   0.41268473 -0.16329667 -0.09031942 -0.01460909]\n [ 0.31392784  0.0243479  -0.32770757  0.22026709  0.23306275 -0.2910481\n  -0.14098494 -0.26040701  0.56467193  0.45277547]\n [ 0.12253963  0.1522326   0.3661648   0.02489514 -0.16148586  0.39053136\n   0.16135947 -0.14183235 -0.04158232 -0.48576695]]\nNumber of epochs: 1342\nFinal Weights \nHidden Layer Weight =  [[-0.01254347  0.27769931  1.01038264  0.64396206  0.44341531 -0.3005272\n   0.54773947 -0.24111447 -0.24108449 -0.25500427]\n [-3.89605929  1.48171119 -1.19444056 -0.97447689 -0.49014644 -1.32382528\n   1.86253133  1.93623577  0.23347265 -2.71596317]\n [-1.53301941  0.68805779 -0.67327923  2.11773687 -0.30757901 -0.12351454\n   3.84156574 -0.79049152 -4.48837974 -2.7468723 ]\n [-3.51853395 -0.17429873  0.8066119  -5.66629989 -0.15135577  0.52182001\n  -1.4358877   1.06888509 -0.53034382  1.42110172]\n [-1.53743534  1.94391105 -0.61165326  4.83973949 -0.02871267 -1.53961499\n   3.91284624 -0.38130997 -5.96400173  0.39501418]\n [ 1.32727592 -1.94661869 -5.84003627  0.03802762 -0.45237566  1.65364713\n  -2.45477329 -0.5626947   1.50276051 -0.41688501]\n [ 0.48800922 -0.2593348  -0.8728992   0.47028897  2.66928136 -1.74756409\n  -0.59720851 -5.6416641  -4.56314149  2.25789349]\n [-1.30687551 -0.03051417 -0.72978021 -0.61794013  1.65149602  0.94043426\n  -3.22476701 -1.60411811 -2.87756461  4.00515611]\n [-1.93598497  2.69258434 -3.61501563 -2.06118795  0.25542633  0.9896243\n  -1.96404619  2.83750714 -0.75598004  0.60843877]\n [ 0.43464492 -1.55031794 -1.01192283 -0.79531825  0.80960542  0.26641097\n  -1.49004538 -0.85280005  2.9864934  -1.82145104]]\nOutput Layer Weight =  [[-1.02010673  0.04846913 -0.40148031  0.13474293  0.20136641  0.25076522\n  -0.02781928 -0.36169814 -0.01065332 -0.27382219]\n [-1.64178322 -1.53141585  0.59774339  0.8448978   0.96690443 -1.52153451\n  -0.93542809 -0.09166736 -0.40560551  1.90697952]\n [-0.46351388 -0.48368548  0.13104939 -3.30081743 -0.60121751 -0.99649408\n   4.44521523 -2.59477387  2.77273033 -0.57693012]\n [ 1.90828827  0.8058356   1.76357955 -1.68016627 -2.7613698   3.75984199\n   0.16207355  0.09556261 -1.05053399 -3.21780832]\n [-1.77853999 -1.14748427  1.80608159 -1.05309447  0.0362738  -2.03165478\n  -2.71499378  3.11111276 -0.48309779  1.76243971]\n [-1.66624466  3.53817606 -4.26589613  2.02638179  2.44894653 -0.37534546\n  -1.30789597  0.67689667 -1.9325488   0.11900879]\n [-0.91171469 -0.4431407   0.01170327 -0.31014539 -0.1882683  -1.41201094\n   0.35520836 -0.6063648   0.59154695  0.6083707 ]\n [-1.02294619 -1.03854369  0.70108238  3.21241745  0.15133211  0.29859719\n   0.12224827 -0.83700928 -0.26224616 -0.46376878]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Training Data Sample","metadata":{}},{"cell_type":"code","source":"d.train_data.head(5)","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"    ID  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n83  84         18         70         18         59         76         56   \n53  54         17         51         54         53         68         69   \n70  71         62         54         58         51         93         10   \n45  46         29         59         89         27         14         81   \n44  45         51         54         53         69         51         43   \n\n    Feature_7  Feature_8  Feature_9  Feature_10  Output  \n83          5         80         63          36       2  \n53         43         86          0          57       4  \n70         44         72         59          12       6  \n45         91         25         91          17       3  \n44         86          0         57          12       4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Feature_1</th>\n      <th>Feature_2</th>\n      <th>Feature_3</th>\n      <th>Feature_4</th>\n      <th>Feature_5</th>\n      <th>Feature_6</th>\n      <th>Feature_7</th>\n      <th>Feature_8</th>\n      <th>Feature_9</th>\n      <th>Feature_10</th>\n      <th>Output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>83</th>\n      <td>84</td>\n      <td>18</td>\n      <td>70</td>\n      <td>18</td>\n      <td>59</td>\n      <td>76</td>\n      <td>56</td>\n      <td>5</td>\n      <td>80</td>\n      <td>63</td>\n      <td>36</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>54</td>\n      <td>17</td>\n      <td>51</td>\n      <td>54</td>\n      <td>53</td>\n      <td>68</td>\n      <td>69</td>\n      <td>43</td>\n      <td>86</td>\n      <td>0</td>\n      <td>57</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>71</td>\n      <td>62</td>\n      <td>54</td>\n      <td>58</td>\n      <td>51</td>\n      <td>93</td>\n      <td>10</td>\n      <td>44</td>\n      <td>72</td>\n      <td>59</td>\n      <td>12</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>46</td>\n      <td>29</td>\n      <td>59</td>\n      <td>89</td>\n      <td>27</td>\n      <td>14</td>\n      <td>81</td>\n      <td>91</td>\n      <td>25</td>\n      <td>91</td>\n      <td>17</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>45</td>\n      <td>51</td>\n      <td>54</td>\n      <td>53</td>\n      <td>69</td>\n      <td>51</td>\n      <td>43</td>\n      <td>86</td>\n      <td>0</td>\n      <td>57</td>\n      <td>12</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Validate Data Sample","metadata":{}},{"cell_type":"code","source":"d.validate_data.head(5)","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"     ID  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n56   57         17         51         54         53         60         69   \n99  100         53         69         43         86         63          0   \n54   55         83         92         11         67          5         96   \n43   44         36          8          3         20         15         19   \n50   51         23         76          5          5         31         18   \n\n    Feature_7  Feature_8  Feature_9  Feature_10  Output  \n56         43         86          0          57       4  \n99         57         12         52          44       2  \n54         21         29         59          89       3  \n43         26         18         70          18       3  \n50         11         56          2          17       5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Feature_1</th>\n      <th>Feature_2</th>\n      <th>Feature_3</th>\n      <th>Feature_4</th>\n      <th>Feature_5</th>\n      <th>Feature_6</th>\n      <th>Feature_7</th>\n      <th>Feature_8</th>\n      <th>Feature_9</th>\n      <th>Feature_10</th>\n      <th>Output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>56</th>\n      <td>57</td>\n      <td>17</td>\n      <td>51</td>\n      <td>54</td>\n      <td>53</td>\n      <td>60</td>\n      <td>69</td>\n      <td>43</td>\n      <td>86</td>\n      <td>0</td>\n      <td>57</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>100</td>\n      <td>53</td>\n      <td>69</td>\n      <td>43</td>\n      <td>86</td>\n      <td>63</td>\n      <td>0</td>\n      <td>57</td>\n      <td>12</td>\n      <td>52</td>\n      <td>44</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>55</td>\n      <td>83</td>\n      <td>92</td>\n      <td>11</td>\n      <td>67</td>\n      <td>5</td>\n      <td>96</td>\n      <td>21</td>\n      <td>29</td>\n      <td>59</td>\n      <td>89</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>44</td>\n      <td>36</td>\n      <td>8</td>\n      <td>3</td>\n      <td>20</td>\n      <td>15</td>\n      <td>19</td>\n      <td>26</td>\n      <td>18</td>\n      <td>70</td>\n      <td>18</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>51</td>\n      <td>23</td>\n      <td>76</td>\n      <td>5</td>\n      <td>5</td>\n      <td>31</td>\n      <td>18</td>\n      <td>11</td>\n      <td>56</td>\n      <td>2</td>\n      <td>17</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Test Data Sample","metadata":{}},{"cell_type":"code","source":"d.test_data.head(5)","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    ID  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n63  64         55         15         85         26         40         24   \n84  85         56          5         80         63         95         36   \n37  38         43         86          0         57         81         12   \n29  30         17         91         87         93         50         15   \n1    2         91         25         91         17         60         91   \n\n    Feature_7  Feature_8  Feature_9  Feature_10  Output  \n63         81          8         14          75       3  \n84         74          9         77          92       6  \n37         52         44         74          64       6  \n29         93         45         17          51       6  \n1          87         93         15          93       5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Feature_1</th>\n      <th>Feature_2</th>\n      <th>Feature_3</th>\n      <th>Feature_4</th>\n      <th>Feature_5</th>\n      <th>Feature_6</th>\n      <th>Feature_7</th>\n      <th>Feature_8</th>\n      <th>Feature_9</th>\n      <th>Feature_10</th>\n      <th>Output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>63</th>\n      <td>64</td>\n      <td>55</td>\n      <td>15</td>\n      <td>85</td>\n      <td>26</td>\n      <td>40</td>\n      <td>24</td>\n      <td>81</td>\n      <td>8</td>\n      <td>14</td>\n      <td>75</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>85</td>\n      <td>56</td>\n      <td>5</td>\n      <td>80</td>\n      <td>63</td>\n      <td>95</td>\n      <td>36</td>\n      <td>74</td>\n      <td>9</td>\n      <td>77</td>\n      <td>92</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>38</td>\n      <td>43</td>\n      <td>86</td>\n      <td>0</td>\n      <td>57</td>\n      <td>81</td>\n      <td>12</td>\n      <td>52</td>\n      <td>44</td>\n      <td>74</td>\n      <td>64</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>30</td>\n      <td>17</td>\n      <td>91</td>\n      <td>87</td>\n      <td>93</td>\n      <td>50</td>\n      <td>15</td>\n      <td>93</td>\n      <td>45</td>\n      <td>17</td>\n      <td>51</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>91</td>\n      <td>25</td>\n      <td>91</td>\n      <td>17</td>\n      <td>60</td>\n      <td>91</td>\n      <td>87</td>\n      <td>93</td>\n      <td>15</td>\n      <td>93</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Accuracy on test data","metadata":{}},{"cell_type":"code","source":"print(\"Holdout Accuracy: \" + str(d.confusion_matrix(d.test_data)*100))\n#print(\"Training Accuracy: \" + str(accuracy(Training_Set)))","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Holdout Accuracy: 50.0\n","output_type":"stream"}]},{"cell_type":"code","source":"d.confusion_matrix_create()","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## HeatMap for Confusion Matrix","metadata":{}},{"cell_type":"code","source":"sns.heatmap(d.confusion_matrix_final,cmap = 'Blues', linewidths=0.5, annot=True)","metadata":{"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f07b0cabd10>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"### Accuracy on Validation Data ","metadata":{}},{"cell_type":"code","source":"print(\"Holdout Accuracy: \" + str(d.confusion_matrix(d.df1)*100))","metadata":{"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Holdout Accuracy: 50.0\n","output_type":"stream"}]}]}